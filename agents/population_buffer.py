from functools import partial

import jax
import jax.numpy as jnp
from flax import struct
import chex

from agents.population_interface import AgentPopulation


def add_partners_to_buffer(population, buffer, params_batch):
    '''
    Add multiple partner checkpoints to the buffer.
    '''
    def add_single_partner(carry_buffer, params):
        return population.add_agent(carry_buffer, params), None
    
    new_buffer, _ = jax.lax.scan(
        add_single_partner,
        buffer,
        params_batch
    )
    return new_buffer

def get_final_buffer(buffer):
    '''
    Get the final buffer from the buffer generated by the last iteration of OEL.
    
    buffer is a PopulationBuffer object with fields 
    params: (max_pop_size, 1, 1, max_pop_size, ...)
    scores: (max_pop_size, 1, max_pop_size)
    ages: (max_pop_size, 1, 1, max_pop_size)
    filled: (max_pop_size, 1, max_pop_size)
    filled_count: (max_pop_size, 1, 1)
    # configuration parameters
    buffer_size: int
    staleness_coef: float
    temp: float
    '''
    # adding extra 1 dimension purely for compatibility with previous implementation
    # TODO: remove this hack once we're done with experiments 
    # which returned the buffer for all OEL iterations.
    last_params = jax.tree.map(lambda x: x[-1][jnp.newaxis], buffer.params)
    last_scores = jax.tree.map(lambda x: x[-1][jnp.newaxis], buffer.scores)
    last_ages = jax.tree.map(lambda x: x[-1][jnp.newaxis], buffer.ages)
    last_filled = jax.tree.map(lambda x: x[-1][jnp.newaxis], buffer.filled)
    last_filled_count = jax.tree.map(lambda x: x[-1][jnp.newaxis], buffer.filled_count)
    return PopulationBuffer(
        params=last_params,
        scores=last_scores,
        ages=last_ages,
        filled=last_filled,
        filled_count=last_filled_count,
        buffer_size=buffer.buffer_size,
        staleness_coef=buffer.staleness_coef,
        temp=buffer.temp,
    )

class PopulationBuffer(struct.PyTreeNode):
    """PyTree structure to store population parameters with fixed buffer size."""
    params: chex.ArrayTree  # Parameters for all agents in the buffer
    scores: chex.Array  # Score for each agent
    ages: chex.Array  # Age (staleness) of each agent
    filled: chex.Array  # Boolean mask for filled slots
    filled_count: chex.Array  # Count of filled slots
    
    # Configuration parameters
    buffer_size: int = struct.field(pytree_node=False, default=100)
    staleness_coef: float = struct.field(pytree_node=False, default=0.3)
    temp: float = struct.field(pytree_node=False, default=1.0)


class BufferedPopulation(AgentPopulation):
    """Population that maintains a buffer of agent parameters that grows over time.
    
    This extends the AgentPopulation class and provides methods to add new agents
    to the buffer and sample from existing agents based on their scores.
    """
    def __init__(self, max_pop_size, policy_cls, sampling_strategy="plr", staleness_coef=0.3, temp=1.0):
        """Initialize a buffered population.
        
        Args:
            max_pop_size: Maximum number of agents in the population
            policy_cls: Agent policy class
            sampling_strategy: Method for sampling partners ('plr' or 'uniform').
            staleness_coef: Weight for staleness in 'plr' sampling. If 0, 
                only the score is used for sampling. If scores are 1s (default), 
                sampling becomes uniform over filled slots (equivalent to 'uniform' strategy).
            temp: Temperature for score-based sampling distribution (currently has no effect).
        """
        super().__init__(max_pop_size, policy_cls)
        self.max_pop_size = max_pop_size
        self.staleness_coef = staleness_coef
        self.temp = temp # Note: Currently ineffective
        self.sampling_strategy = sampling_strategy
        assert sampling_strategy in ["plr", "uniform"], "sampling_strategy must be 'plr' or 'uniform'"
    
    @partial(jax.jit, static_argnums=(0,))
    def reset_buffer(self, example_params):
        """Initialize the buffer with zeros of the appropriate shape.
        
        Args:
            example_params: Example parameters to determine shape and dtype
            
        Returns:
            A new PopulationBuffer
        """
        # Initialize buffer with zeros like example_params
        params = jax.tree_map(
            lambda x: jnp.zeros((self.max_pop_size,) + x.shape, x.dtype),
            example_params
        )
        
        return PopulationBuffer(
            params=params,
            scores=jnp.ones(self.max_pop_size),  # Default score of 1
            ages=jnp.zeros(self.max_pop_size, dtype=jnp.int32),
            filled=jnp.zeros(self.max_pop_size, dtype=bool),
            filled_count=jnp.zeros(1, dtype=jnp.int32),
            buffer_size=self.max_pop_size,
            staleness_coef=self.staleness_coef,
            temp=self.temp,
        )
    

    @partial(jax.jit, static_argnums=(0,))
    def _get_uniform_sampling_dist(self, buffer):
        """Get uniform distribution over filled slots."""
        filled_count = jnp.maximum(buffer.filled.sum(), 1) # Avoid division by zero
        return buffer.filled / filled_count
    
    @partial(jax.jit, static_argnums=(0,))
    def _get_plr_sampling_dist(self, buffer):
        """Get distribution for sampling agents based on scores and staleness (PLR strategy).
        
        Args:
            buffer: The population buffer
            
        Returns:
            Distribution over agents
        """
        # Score distribution (use scores only for filled slots)
        # TODO: temp currently doesn't do anything. check the PLR paper for correct implementation.
        score_dist = buffer.scores * buffer.filled / buffer.temp
        score_sum = score_dist.sum()  # sum of scores over filled slots
        # if filled slots exist, use score distribution. otherwise, uniform over filled slots
        score_dist = jnp.where(
            score_sum > 0,
            score_dist / score_sum,
            buffer.filled / jnp.maximum(buffer.filled.sum(), 1)  # Uniform over filled slots
        )
        
        # Staleness distribution
        staleness_scores = buffer.ages * buffer.filled
        staleness_sum = staleness_scores.sum()
        staleness_dist = jnp.where(
            staleness_sum > 0,
            staleness_scores / staleness_sum,
            score_dist  # If no staleness, use score distribution
        )
        
        # Combined distribution
        return (1 - buffer.staleness_coef) * score_dist + buffer.staleness_coef * staleness_dist
    
    @partial(jax.jit, static_argnums=(0,))
    def _get_next_insert_idx(self, buffer):
        """Get index for next insertion (either first empty or replace lowest-scored).
        
        Args:
            buffer: The population buffer
            
        Returns:
            Index to insert the next agent
        """
        # Select distribution based on the instance's sampling strategy
        sampling_dist = jax.lax.cond(
            self.sampling_strategy == "uniform",
            self._get_uniform_sampling_dist,
            self._get_plr_sampling_dist,
            buffer # Pass buffer as the operand to the selected function
        )

        return jax.lax.cond(
            jnp.less(buffer.filled_count[0], buffer.buffer_size),
            lambda: buffer.filled_count[0],
            lambda: jnp.argmin(sampling_dist)
        )
    
    @partial(jax.jit, static_argnums=(0,))
    def add_agent(self, buffer, new_params, score=None):
        """Add a new agent to the buffer.
        
        Args:
            buffer: The population buffer
            new_params: Parameters of the new agent to add
            score: Optional score for the new agent (default to 1.0)
            
        Returns:
            Updated population buffer
        """
        # Find index to insert
        insert_idx = self._get_next_insert_idx(buffer)
        
        # Default score
        if score is None:
            score = 1.0
        
        # Update parameters, filled, count, score, age
        new_buffer_params = jax.tree_map(
            lambda x, p: x.at[insert_idx].set(p),
            buffer.params, new_params
        )
        
        new_filled = buffer.filled.at[insert_idx].set(True)
        new_filled_count = jnp.minimum(buffer.filled_count + 1, jnp.array([buffer.buffer_size]))
        new_scores = buffer.scores.at[insert_idx].set(score)
        
        # Reset age for new agent, increment for others
        ages_plus_one = (buffer.ages + 1) * buffer.filled
        new_ages = ages_plus_one.at[insert_idx].set(0)
        
        return buffer.replace(
            params=new_buffer_params,
            filled=new_filled,
            filled_count=new_filled_count,
            scores=new_scores,
            ages=new_ages
        )
    
    @partial(jax.jit, static_argnums=(0, 2))
    def sample_agent_indices(self, buffer, n, rng, needs_resample_mask=None):
        """Sample n agent indices from the buffer based on scores and staleness.
           Optionally takes a mask to indicate which samples are actually used,
           for correct age updates.

        Args:
            buffer: The population buffer
            n: Number of indices to sample (must be static, typically num_envs)
            rng: Random key
            needs_resample_mask: Optional boolean mask of shape (n,) indicating
                                 which sampled indices are actually used.
                                 If None, assumes all n are used.
            sampling_strategy: Sampling strategy to use. If None, uses the instance's sampling strategy.

        Returns:
            indices: Indices of sampled agents (shape (n,))
            new_buffer: Updated buffer with correctly incremented ages
        """
        rng, sample_rng = jax.random.split(rng, 2)

        buffer_has_agents = jnp.greater(buffer.filled.sum(), 0)
        
        # Select distribution based on the instance's sampling strategy
        sampling_dist = jax.lax.cond(
            self.sampling_strategy == "uniform",
            self._get_uniform_sampling_dist,
            self._get_plr_sampling_dist,
            buffer # Pass buffer as the operand to the selected function
        )
        

        def handle_empty_buffer():
            # If buffer is empty, return random indices and unchanged buffer
            rand_indices = jax.random.randint(sample_rng, (n,), 0, buffer.buffer_size)
            return rand_indices, buffer

        def handle_filled_buffer():
            # Always sample n indices using the chosen distribution
            indices = jax.random.choice(
                sample_rng, jnp.arange(buffer.buffer_size),
                shape=(n,), p=sampling_dist, replace=True
            )

            def update_ages_plr(current_buffer, sampled_indices, needs_mask):
                '''
                Update the ages of the buffer based on the sampled indices.
                '''
                # Initialize reset mask for buffer entries
                reset_mask = jnp.zeros(current_buffer.buffer_size, dtype=bool)
                # Default to assuming all samples are used if mask is not provided
                actual_needs_mask = needs_mask if needs_mask is not None else jnp.ones(n, dtype=bool)
                # Loop through samples to build the reset_mask without dynamic slicing
                def update_reset_mask(i, current_reset_mask):
                    buffer_idx = sampled_indices[i]
                    is_needed = actual_needs_mask[i]
                    new_reset_mask = jax.lax.cond(
                        is_needed, lambda mask: mask.at[buffer_idx].set(True), lambda mask: mask, current_reset_mask
                    )
                    return new_reset_mask
                reset_mask = jax.lax.fori_loop(0, n, update_reset_mask, reset_mask)
                # Compute final ages
                increment_mask = current_buffer.filled & (~reset_mask)
                new_ages = current_buffer.ages
                new_ages = jnp.where(increment_mask, new_ages + 1, new_ages)
                new_ages = jnp.where(reset_mask, 0, new_ages)
                return current_buffer.replace(ages=new_ages)

            def skip_age_update(current_buffer, sampled_indices, needs_mask):
                '''Do nothing, return buffer unchanged
                '''
                return current_buffer

            # Apply age updates only for 'plr' strategy to skip for-i loop if possible.
            updated_buffer = jax.lax.cond(
                self.sampling_strategy == "uniform",
                skip_age_update,
                update_ages_plr,
                # Operands for the conditional functions:
                buffer, indices, needs_resample_mask 
            )
            
            return indices, updated_buffer

        indices, new_buffer = jax.lax.cond(
            buffer_has_agents,
            handle_filled_buffer,
            handle_empty_buffer
        )

        return indices, new_buffer
    
    def gather_agent_params(self, buffer: PopulationBuffer, agent_indices):
        """Gather the parameters of agents specified by agent_indices.
        
        Args:
            buffer: The population buffer
            agent_indices: Indices with shape (num_envs,), each in [0, buffer_size)
            
        Returns:
            Gathered parameters with shape (num_envs, ...)
        """
        def gather_leaf(leaf):
            # leaf shape: (buffer_size, ...)
            return jax.vmap(lambda idx: leaf[idx])(agent_indices)
        return jax.tree_map(gather_leaf, buffer.params)
    
    def get_actions(self, buffer, agent_indices, obs, done, avail_actions, hstate, rng, 
                    env_state=None, aux_obs=None, test_mode=False):
        """Get actions from agents in the buffer.
        
        Args:
            buffer: The population buffer 
            agent_indices: Indices with shape (num_envs,), each in [0, buffer_size)
            obs: Observations with shape (num_envs, ...)
            done: Done flags with shape (num_envs,)
            avail_actions: Available actions with shape (num_envs, num_actions)
            hstate: Hidden state with shape (1, num_envs, ...) or None
            rng: Random key
            env_state: Environment state with shape (num_envs, ...) or None
            aux_obs: Optional auxiliary vector to append to observation
            test_mode: Whether to use test mode (deterministic actions)
            
        Returns:
            actions: Actions with shape (num_envs,)
            new_hstate: New hidden state with shape (num_envs, ...) or None
        """
        gathered_params = self.gather_agent_params(buffer, agent_indices)
        num_envs = agent_indices.squeeze().shape[0]
        rngs_batched = jax.random.split(rng, num_envs)
        
        vmapped_get_action = jax.vmap(partial(self.policy_cls.get_action, 
                                             aux_obs=aux_obs, 
                                             env_state=env_state, 
                                             test_mode=test_mode))
        
        # HACK FLAG: reshaping obs to be what an S5 agent expects. No idea if this will work with other policies.
        obs_reshaped = jax.tree.map(lambda x: x.reshape(num_envs, 1, 1, -1), obs)
        done_reshape = jax.tree.map(lambda x: x.reshape(num_envs, 1, 1), done)
        hstate_reshape = jax.tree.map(lambda x: x.reshape(num_envs, 1, 1, -1), hstate)

        actions, new_hstate = vmapped_get_action(
            gathered_params, obs_reshaped, done_reshape, avail_actions, 
            hstate_reshape, rngs_batched)
        
        hstate_shape_back = jax.tree.map(lambda x: x.reshape(1, num_envs, -1), new_hstate)

        return actions, hstate_shape_back 